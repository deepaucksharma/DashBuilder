# Production-Ready OpenTelemetry Collector Configuration
# This replaces the problematic single-file configuration with a modular, efficient approach

# Extensions for production features
extensions:
  # Health monitoring
  health_check:
    endpoint: 0.0.0.0:13133
    path: "/health"
    check_collector_pipeline:
      enabled: true
      interval: 5s
      exporter_failure_threshold: 5
  
  # Performance profiling
  pprof:
    endpoint: 0.0.0.0:1777
    block_profile_fraction: 3
    mutex_profile_fraction: 5
  
  # State persistence
  file_storage:
    directory: /var/lib/nrdot/state
    timeout: 5s
    fsync: true
  
  # Memory monitoring
  memory_ballast:
    size_mib: 512  # Prevent aggressive GC

# Receivers with proper limits
receivers:
  # Host metrics with bounded collection
  hostmetrics:
    collection_interval: 60s
    scrapers:
      cpu:
        metrics:
          system.cpu.time:
            enabled: true
          system.cpu.utilization:
            enabled: true
      
      memory:
        metrics:
          system.memory.usage:
            enabled: true
          system.memory.utilization:
            enabled: true
      
      process:
        # Critical: Limit process discovery
        max_processes: 1000
        
        # Smart sampling
        sampling:
          enabled: true
          initial_delay: 10s
          sampling_interval: 30s
          threshold_cpu_percent: 0.1
          threshold_memory_mb: 10
        
        # Resource limits for the scraper itself
        resource_limits:
          max_cpu_percent: 5
          max_memory_mb: 500
          max_file_handles: 1000
        
        # Multi-criteria exclusion
        exclude:
          match_type: regexp
          names:
            - "kworker/.*"
            - "ksoftirqd/.*"
            - "migration/.*"
            - "rcu_.*"
            - ".*\\[.*\\]"  # Kernel threads
          
        # Additional exclusion criteria
        exclude_criteria:
          cpu_below_percent: 0.01
          memory_below_mb: 1
          lifetime_below_seconds: 60
          state_not_in: ["running", "sleeping"]
        
        metrics:
          process.cpu.time:
            enabled: true
          process.memory.physical:
            enabled: true
          process.memory.virtual:
            enabled: true
          process.disk.io:
            enabled: true
  
  # Self-monitoring
  prometheus:
    config:
      scrape_configs:
        - job_name: 'otel-collector'
          scrape_interval: 10s
          static_configs:
            - targets: ['0.0.0.0:8888']

# Processors for efficient pipeline
processors:
  # Memory protection with progressive degradation
  memory_limiter:
    check_interval: 100ms
    limit_percentage: 75
    spike_limit_percentage: 25
    
    # Progressive degradation levels
    degradation_levels:
      - threshold_percentage: 60
        actions:
          - drop_metrics_where: 'attributes["importance_score"] < 0.3'
          - increase_batch_timeout: 2x
      
      - threshold_percentage: 70
        actions:
          - drop_metrics_where: 'attributes["importance_score"] < 0.6'
          - reduce_export_concurrency: 50%
      
      - threshold_percentage: 80
        actions:
          - sample_metrics_where: 'attributes["importance_score"] < 0.9'
          - sample_rate: 0.5
          - pause_new_scrapers: true
      
      - threshold_percentage: 90
        actions:
          - keep_only_metrics_where: 'attributes["importance_score"] >= 0.9'
          - force_batch_export: true
          - alert: "critical_memory_pressure"
  
  # Efficient classification without OTTL overhead
  attributes/classify:
    actions:
      # Pre-compute importance scores
      - key: importance_score
        action: upsert
        value: |
          Switch(
            Case(MatchesPattern(process.executable.name, ".*database.*"), 0.9),
            Case(MatchesPattern(process.executable.name, ".*api.*"), 0.8),
            Case(MatchesPattern(process.executable.name, ".*web.*"), 0.7),
            Case(MatchesPattern(process.executable.name, ".*worker.*"), 0.6),
            Case(MatchesPattern(process.executable.name, ".*cache.*"), 0.5),
            Default(0.3)
          )
      
      # Add process class for routing
      - key: process_class
        action: upsert
        value: |
          Switch(
            Case(importance_score >= 0.9, "critical"),
            Case(importance_score >= 0.6, "important"),
            Case(importance_score >= 0.3, "standard"),
            Default("low")
          )
  
  # Stateful EWMA calculation with persistence
  ewma/stateful:
    storage: file_storage
    alpha: 0.1
    warmup_intervals: 5
    metrics:
      - metric_name: process.cpu.time
        group_by: [process.executable.name]
      - metric_name: process.memory.physical
        group_by: [process.executable.name]
    
    anomaly_detection:
      enabled: true
      threshold_stddev: 3.0
      min_samples: 10
  
  # Efficient routing instead of filtering
  routing/by_importance:
    from_attribute: process_class
    drop_unmatched: true
    table:
      - value: "critical"
        pipelines: [metrics/critical]
      - value: "important"
        pipelines: [metrics/important]
      - value: "standard"
        pipelines: [metrics/standard]
      - value: "low"
        pipelines: [metrics/sampled]
  
  # Optimized batching for different priorities
  batch/critical:
    send_batch_size: 1000
    timeout: 1s
    send_batch_max_size: 2000
  
  batch/standard:
    send_batch_size: 5000
    timeout: 10s
    send_batch_max_size: 10000
  
  batch/sampled:
    send_batch_size: 10000
    timeout: 30s
    send_batch_max_size: 20000
  
  # Probabilistic sampling for low-importance metrics
  probabilistic_sampler/low:
    sampling_percentage: 10
    hash_seed: 22
  
  # Metric transformation for aggregation
  metricstransform/aggregate:
    transforms:
      - include: process.cpu.time
        match_type: regexp
        action: aggregate
        group_by:
          - service.name
          - host.name
        aggregation_type: sum
      
      - include: process.memory.*
        match_type: regexp
        action: aggregate  
        group_by:
          - service.name
          - host.name
        aggregation_type: max

# Exporters with high availability
exporters:
  # Primary exporter with circuit breaker
  otlphttp/primary:
    endpoint: "${NEW_RELIC_OTLP_ENDPOINT}"
    headers:
      api-key: "${NEW_RELIC_LICENSE_KEY}"
    
    # Compression for bandwidth efficiency
    compression: zstd
    
    # Retry configuration with circuit breaker
    retry_on_failure:
      enabled: true
      initial_interval: 1s
      max_interval: 30s
      max_elapsed_time: 300s
      multiplier: 2
      randomization_factor: 0.5
    
    # Circuit breaker settings
    circuit_breaker:
      enabled: true
      failure_threshold: 5
      recovery_timeout: 30s
      half_open_requests: 3
    
    # Connection pooling
    max_idle_conns: 100
    max_conns_per_host: 10
    idle_conn_timeout: 90s
    
    # Queuing for resilience
    sending_queue:
      enabled: true
      num_consumers: 10
      queue_size: 10000
      storage: file_storage
  
  # Fallback exporter for critical metrics
  otlphttp/fallback:
    endpoint: "${NEW_RELIC_OTLP_ENDPOINT_FALLBACK}"
    headers:
      api-key: "${NEW_RELIC_LICENSE_KEY}"
    compression: gzip
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 60s
  
  # Debug exporter for troubleshooting
  debug:
    verbosity: detailed
    sampling_initial: 10
    sampling_thereafter: 100
  
  # Prometheus exporter for self-monitoring
  prometheus:
    endpoint: "0.0.0.0:9090"
    const_labels:
      collector_id: "${COLLECTOR_ID}"
      environment: "${ENVIRONMENT}"

# Service pipelines - Multi-stage architecture
service:
  # Enable extensions
  extensions: 
    - health_check
    - pprof
    - file_storage
    - memory_ballast
  
  # Telemetry configuration
  telemetry:
    logs:
      level: info
      output_paths: ["/var/log/nrdot/collector.log"]
      error_output_paths: ["/var/log/nrdot/collector_error.log"]
    
    metrics:
      level: detailed
      address: 0.0.0.0:8888
      metric_readers:
        - periodic:
            interval: 10s
            exporter:
              prometheus:
                host: 0.0.0.0
                port: 9090
  
  # Multi-stage pipelines
  pipelines:
    # Stage 1: Ingestion and Classification
    metrics/ingestion:
      receivers: [hostmetrics]
      processors:
        - memory_limiter
        - attributes/classify
      exporters: [routing/by_importance]
    
    # Stage 2A: Critical Metrics (Low Latency)
    metrics/critical:
      receivers: [routing/by_importance]
      processors:
        - ewma/stateful
        - batch/critical
      exporters: 
        - otlphttp/primary
        - otlphttp/fallback
    
    # Stage 2B: Important Metrics (Balanced)
    metrics/important:
      receivers: [routing/by_importance]
      processors:
        - ewma/stateful
        - metricstransform/aggregate
        - batch/standard
      exporters: [otlphttp/primary]
    
    # Stage 2C: Standard Metrics (Cost Optimized)
    metrics/standard:
      receivers: [routing/by_importance]
      processors:
        - metricstransform/aggregate
        - batch/standard
      exporters: [otlphttp/primary]
    
    # Stage 2D: Sampled Metrics (Deep Analytics)
    metrics/sampled:
      receivers: [routing/by_importance]
      processors:
        - probabilistic_sampler/low
        - batch/sampled
      exporters: [otlphttp/primary]
    
    # Self-monitoring pipeline
    metrics/monitoring:
      receivers: [prometheus]
      processors:
        - batch/critical
      exporters: 
        - prometheus
        - otlphttp/primary